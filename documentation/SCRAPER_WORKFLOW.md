# Scraper System Workflow

## Visual Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER ADDS PRODUCT                        â”‚
â”‚                   (e.g., Amazon product URL)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PRODUCT SAVED TO DB                        â”‚
â”‚                   (with URL and scan frequency)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SCHEDULER RUNS (15 min)                     â”‚
â”‚               Checks which products need scanning               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRICE SCANNER SERVICE                        â”‚
â”‚              Loops through products due for scan                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SCRAPER FACTORY                            â”‚
â”‚            Analyzes URL â†’ Selects Scraper                       â”‚
â”‚                                                                 â”‚
â”‚  amazon.com     â†’ AmazonScraper                                 â”‚
â”‚  ebay.com       â†’ EbayScraper                                   â”‚
â”‚  newegg.com     â†’ NeweggScraper                                 â”‚
â”‚  other sites    â†’ GenericScraper                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SELECTED SCRAPER                            â”‚
â”‚                                                                 â”‚
â”‚  1. fetch_page()  â†’ Get HTML from website                       â”‚
â”‚  2. parse HTML    â†’ Extract price & stock                       â”‚
â”‚  3. return data   â†’ {price, in_stock, currency}                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SAVE TO PRICE HISTORY                         â”‚
â”‚            (timestamp, price, stock status)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    UPDATE DASHBOARD                             â”‚
â”‚        User sees latest price, trends, statistics              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Code Flow

### 1. Scheduler Triggers Scan
```python
# backend/app/scheduler.py
def scan_prices_job():
    result = PriceScannerService.scan_all_due_products(db)
```

### 2. Scanner Service Gets Products
```python
# backend/app/services/price_scanner.py
async def scan_product(product, db):
    result = await ScraperFactory.scrape_url(product.url)
    # Save to database
```

### 3. Factory Selects Scraper
```python
# backend/app/scrapers/scraper_factory.py
def create_scraper(url):
    domain = urlparse(url).netloc
    
    if 'amazon.com' in domain:
        return AmazonScraper(url)
    elif 'ebay.com' in domain:
        return EbayScraper(url)
    # etc...
    
    return GenericScraper(url)
```

### 4. Scraper Extracts Data
```python
# backend/app/scrapers/amazon_scraper.py
async def scrape(self):
    html = await self.fetch_page()
    soup = BeautifulSoup(html, 'lxml')
    
    price = self._extract_price(soup)
    in_stock = self._check_stock(soup)
    
    return {
        "price": price,
        "in_stock": in_stock,
        "currency": "USD"
    }
```

### 5. Data Saved to Database
```python
# backend/app/services/price_scanner.py
price_history = PriceHistory(
    product_id=product.id,
    price=result['price'],
    currency=result['currency'],
    in_stock=result['in_stock'],
    scraped_at=datetime.utcnow()
)
db.add(price_history)
db.commit()
```

## Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Products â”‚ (products table)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ has many
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PriceHistory â”‚ (price_history table)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ Each record contains:
     â”‚ - product_id
     â”‚ - price
     â”‚ - currency
     â”‚ - in_stock
     â”‚ - scraped_at (timestamp)
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dashboard   â”‚ (API aggregates data)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ Shows:
     â”‚ - Current price
     â”‚ - Price changes
     â”‚ - Trend charts
     â”‚ - Statistics
```

## Scraper Class Hierarchy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BaseScraper    â”‚ (Abstract Base Class)
â”‚                 â”‚
â”‚  + fetch_page() â”‚ â† Fetches HTML
â”‚  + parse_price()â”‚ â† Extracts numbers from text
â”‚  + scrape()     â”‚ â† Abstract (must implement)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ extends
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                     â”‚
    â–¼                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚GenericScraperâ”‚                 â”‚ Vendor-Specific â”‚
â”‚              â”‚                 â”‚    Scrapers     â”‚
â”‚ Uses common  â”‚                 â”‚                 â”‚
â”‚ CSS patterns â”‚                 â”‚ â€¢ AmazonScraper â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚ â€¢ EbayScraper   â”‚
                                 â”‚ â€¢ NeweggScraper â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Adding a New Scraper (Step by Step)

```
Step 1: Create File
â””â”€> backend/app/scrapers/walmart_scraper.py

Step 2: Write Class
â””â”€> class WalmartScraper(BaseScraper):
        async def scrape(self): ...

Step 3: Import in Factory
â””â”€> backend/app/scrapers/scraper_factory.py
    from .walmart_scraper import WalmartScraper

Step 4: Register Domain
â””â”€> elif 'walmart.com' in domain:
        return WalmartScraper(url)

Step 5: Test
â””â”€> python test_scraper.py https://walmart.com/product

Step 6: Use Automatically
â””â”€> System now auto-selects WalmartScraper
    for all walmart.com URLs!
```

## Request Flow (Detailed)

```
User clicks "Scan All Products"
        â”‚
        â–¼
POST /api/scanner/scan-all
        â”‚
        â–¼
PriceScannerService.scan_all_due_products()
        â”‚
        â”œâ”€> Get all active products from DB
        â”‚
        â”œâ”€> Filter products due for scan
        â”‚   (based on last_scanned_at + scan_frequency)
        â”‚
        â””â”€> For each product:
                â”‚
                â”œâ”€> ScraperFactory.scrape_url(product.url)
                â”‚       â”‚
                â”‚       â”œâ”€> Parse domain from URL
                â”‚       â”‚
                â”‚       â”œâ”€> Select scraper class
                â”‚       â”‚   - AmazonScraper if amazon.com
                â”‚       â”‚   - EbayScraper if ebay.com
                â”‚       â”‚   - GenericScraper otherwise
                â”‚       â”‚
                â”‚       â”œâ”€> scraper.fetch_page()
                â”‚       â”‚   - HTTP GET request
                â”‚       â”‚   - Returns HTML
                â”‚       â”‚
                â”‚       â”œâ”€> scraper.scrape()
                â”‚       â”‚   - Parse HTML with BeautifulSoup
                â”‚       â”‚   - Find price elements
                â”‚       â”‚   - Find stock status
                â”‚       â”‚   - Return dict
                â”‚       â”‚
                â”‚       â””â”€> Return {price, in_stock, currency}
                â”‚
                â”œâ”€> Create PriceHistory record
                â”‚   - product_id
                â”‚   - price
                â”‚   - currency
                â”‚   - in_stock
                â”‚   - scraped_at (now)
                â”‚
                â”œâ”€> Update product.last_scanned_at
                â”‚
                â””â”€> Save to database
```

## Error Handling Flow

```
Try to scrape product
        â”‚
        â”œâ”€ HTTP Error (403, 404, etc.)
        â”‚   â””â”€> Log warning
        â”‚       â””â”€> Return {price: None, in_stock: False}
        â”‚
        â”œâ”€ Parsing Error (element not found)
        â”‚   â””â”€> Try fallback selectors
        â”‚       â”œâ”€> Success? â†’ Return data
        â”‚       â””â”€> Fail? â†’ Return defaults
        â”‚
        â”œâ”€ Timeout
        â”‚   â””â”€> Log error
        â”‚       â””â”€> Return defaults
        â”‚
        â””â”€ Any other exception
            â””â”€> Log error with traceback
                â””â”€> Return defaults
                    â””â”€> Continue with next product
```

## Configuration Points

### Scan Frequency
```python
# Per-product setting (default: 60 minutes)
product.scan_frequency_minutes = 120  # Check every 2 hours
```

### Global Scheduler
```python
# backend/app/scheduler.py
# Check for due products every 15 minutes
scheduler.add_job(
    scan_prices_job,
    trigger=IntervalTrigger(minutes=15),
    ...
)
```

### Rate Limiting (BaseScraper)
```python
# Minimum delay between requests to same domain
_min_delay = 2  # seconds
```

## Performance Optimization

```
Sequential Scanning
â”œâ”€ Product 1 â†’ Scrape â†’ Save
â”œâ”€ Product 2 â†’ Scrape â†’ Save
â”œâ”€ Product 3 â†’ Scrape â†’ Save
â””â”€ Takes: N Ã— (scrape_time + save_time)

Concurrent Scanning (Current Implementation)
â”œâ”€ Product 1 â”
â”œâ”€ Product 2 â”œâ”€ All scrape concurrently
â”œâ”€ Product 3 â”˜
â””â”€ Takes: max(scrape_time) + save_time

Benefits:
â€¢ Faster total scan time
â€¢ Better resource utilization
â€¢ Still respects rate limits per domain
```

## Maintenance Checklist

```
Weekly:
â–¡ Check for scraper failures in logs
â–¡ Test a few products manually

Monthly:
â–¡ Review which scrapers are most used
â–¡ Update selectors if sites changed
â–¡ Add new vendor scrapers if needed

When Adding Products:
â–¡ Test URL with test_scraper.py first
â–¡ Verify price and stock detection
â–¡ Adjust scan frequency as needed
```

---

This workflow ensures accurate, reliable price tracking across multiple vendors! ğŸš€

